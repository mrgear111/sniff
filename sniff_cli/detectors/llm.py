from typing import Dict
from anthropic import Anthropic
import os
import re
from dotenv import load_dotenv

load_dotenv()

class LLMAnalyzer:
    def __init__(self):
        # We assume the user has set ANTHROPIC_API_KEY in their environment, or loaded via dotenv
        self.api_key = os.environ.get("ANTHROPIC_API_KEY")
        self.client = None
        if self.api_key:
            self.client = Anthropic(api_key=self.api_key)

    def analyze(self, diff: str, message: str) -> Dict[str, any]:
        """
        Sends the commit diff and message to Claude 4.6 Sonnet for a deterministic AI-detection tie-breaker.
        """
        if not self.client:
            return {
                "score": -1.0, 
                "reason": "Anthropic API key not found. LLM secondary verification skipped."
            }

        # Truncate extremely large diffs to avoid token limit explosions, preserving the top and bottom
        if len(diff) > 15000:
            diff = diff[:7500] + "\n\n... [DIFF TRUNCATED] ...\n\n" + diff[-7500:]

        prompt = f"""
You are an expert AI code detection engine. 
Analyze the following Git commit diff and message. Determine if the code was likely written by a human or generated by an AI (like Copilot, Claude, or ChatGPT).

Humans write chaotic, organically imperfect code, often with debug statements, typos in comments, and inconsistent spacing.
AI generates unnaturally perfect, boilerplate-heavy, syntactically uniform code with generic commit messages.

Commit Message:
{message}

Diff:
{diff}

Output ONLY a raw JSON mapping with two keys:
1. "score": A float between 0.0 (absolutely human) and 1.0 (absolutely AI).
2. "reason": A single, concise, brutal sentence explaining the strongest signal for your score.
"""
        try:
            response = self.client.messages.create(
                model="claude-sonnet-4-6",
                max_tokens=200,
                temperature=0.0,
                system="You are a strict code analysis JSON API. Always output raw JSON.",
                messages=[
                    {"role": "user", "content": prompt}
                ]
            )
            
            raw_response = response.content[0].text.strip()
            
            # Extract JSON block securely
            json_match = re.search(r'\{.*\}', raw_response, re.DOTALL)
            if json_match:
                import json
                result = json.loads(json_match.group(0))
                # Ensure clamped bounds
                score = max(0.0, min(1.0, float(result.get("score", 0.0))))
                reason = str(result.get("reason", "LLM provided no reason."))
                return {"score": score, "reason": f"{reason} [ðŸ¤– Verified by Claude]"}
            else:
                return {"score": -1.0, "reason": "Claude API returned invalid JSON layout."}

        except Exception as e:
            return {"score": -1.0, "reason": f"Claude API call failed: {str(e)}"}
